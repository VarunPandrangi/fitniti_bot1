// server.js
import express      from 'express';
import cors         from 'cors';
import fs           from 'fs';
import dotenv       from 'dotenv';
import OpenAI       from 'openai';
import cosineSim    from 'cosine-similarity';

dotenv.config();
const app = express();
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// Load embeddings generated by Ollama
const STORE = JSON.parse(fs.readFileSync('embeddings.json'));

// 1) Initialize a client to connect to your local Ollama server
const ollama = new OpenAI({
    baseURL: process.env.OLLAMA_HOST + '/v1',
    apiKey: 'ollama', // required but not used
});


// 2) Helper: embed a query via local Ollama
async function embedQuery(text) {
    console.log("Embedding query with Ollama...");
    const resp = await ollama.embeddings.create({
        model: 'nomic-embed-text',
        input: text
    });
    return resp.data[0].embedding;
}


// 3) Retrieve topâ€3 most similar docs (Now with better logging and safety checks)
async function retrieveContext(query) {
    const qEmb = await embedQuery(query);

    // --- DEBUGGING LOGS ---
    console.log(`Query embedding type: ${typeof qEmb}, IsArray: ${Array.isArray(qEmb)}, Length: ${qEmb?.length}`);
    if (STORE.length > 0) {
        console.log(`First doc embedding type: ${typeof STORE[0].embedding}, IsArray: ${Array.isArray(STORE[0].embedding)}, Length: ${STORE[0].embedding?.length}`);
    }
    // --- END DEBUGGING ---

    const scored = STORE.map(d => {
        let score = 0; // Default score
        // Ensure both embeddings are valid arrays before comparing
        if (Array.isArray(qEmb) && Array.isArray(d.embedding)) {
            score = cosineSim(qEmb, d.embedding);
        } else {
            console.error('Skipping a document due to invalid embedding format.');
        }
        return { text: d.text, score: score };
    });

    // Sort by score, handling potential non-numeric values gracefully
    scored.sort((a, b) => (b.score || 0) - (a.score || 0));

    // Safely format the scores for logging to prevent crashes
    const topScoresFormatted = scored.slice(0, 3).map(s => {
        if (typeof s.score === 'number') {
            return s.score.toFixed(4);
        }
        return 'N/A'; // Return 'N/A' if score is not a number
    }).join(', ');

    console.log(`Top 3 scores: ${topScoresFormatted}`);

    return scored.slice(0, 3).map(x => x.text);
}


// 4) Chat endpoint using Ollama
app.post('/api/chat', async (req, res) => {
    try {
        const userMsg  = req.body.message;
        const contexts = await retrieveContext(userMsg);

        const systemPrompt = `
You are FitBot, the Fitniti Runnerâ€™s Portal assistant.
Use ONLY these Fitniti knowledge snippets to answer the user's question:
${contexts.join('\n\n---\n\n')}
---
Greet the user once. Scope your answer to Marathon Management, Corporate Challenges, or our Charity Platform.
If the question is out of scope, politely deflect.
Always end your response with a call-to-action to visit our contact page for more details.
        `.trim();

        console.log("Calling local Ollama Chat API...");
        const completion = await ollama.chat.completions.create({
            model: 'llama3',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user',   content: userMsg }
            ],
            stream: false
        });

        const reply = completion.choices[0].message.content;
        res.json({ reply });

    } catch (err) {
        console.error('Ollama API Error:', err.message);
        res.status(500).json({ reply: 'Sorry, something went wrong with the local AI service.' });
    }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`ðŸš€ Server running at http://localhost:${PORT}`));
